setup:
  # Chooses whether to benchmark kernels
  # TODO Disabled for now since the input data will have variable sizes
  # TODO check if it is rerunning benchmarks
  benchmark_kernels: false
  # Sets precision for float32 operations in CUDA matmul and cuDNN convolutions
  # 'highest' disables tf32
  # 'high' and 'medium' enable tf32
  tf32_level: high
  amp:
    # Enables/disables AMP
    enabled: true
    # Sets AMP data type
    dtype: [(obj), torch.bfloat16]
  grad_manager:
    scaler:
      # Enables/disables grad scaling
      enabled: false
      batch_retry:
        # Enables/disables batch retrying
        enabled: false
        # Sets maximum number of retries for a batch before giving up
        max_retries: 4
  # Seed for computations
  seed: 42
  distributed:
    # Number of threads running in each process, already computed from cpu count and topology
    #num_threads: null
    # Torchrun already sets local and global ranks as environment variables
    # Number of process in the process group
    world_size: [(env), WORLD_SIZE, 1]
    # Number of processes locally
    local_world_size: [(env), LOCAL_WORLD_SIZE, 1]
    # Id of current process globally
    rank: [(env), RANK, 0]
    # Id of current process locally
    local_rank: [(env), LOCAL_RANK, 0]
    # Timeout for ddp process group
    timeout: 1800
  # Device used in current process, computed in code
  #device: null

  # Trainer object to run
  trainer_constructor: [(obj), src.trainer.DVSTTrainer]

train:
  optimizer:
    # Learning rate
    lr: 1e-4
    # AdamW betas
    betas: [0.9, 0.95]
  total_epochs: 1
  save_every_passes: 100
  checkpoint_folder_path: res/tmp/checkpoint/
  grad_clipping:
    # Whether to use gradient clipping
    enabled: true
    # Maximum gradient norm for gradient clipping
    max_norm: 1.0

  data:
    dataloader:
      batch_size: null
      num_workers: 4 # TODO 4 per gpu, needs to be checked (source: https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5)
      prefetch_factor: null
      shuffle: true

    # TODO Add MultiShapeNet dataset https://srt-paper.github.io/#dataset
    # TODO Add RealEstate10K dataset processed by pixelsplat https://github.com/dcharatan/pixelsplat/blob/main/README.md#acquiring-datasets
    #   Original: google.github.io/realestate10k/download.html
    #   Easier script to download (supposedly): https://github.com/Findeton/real-state-10k
    # when adding image datasets, concat them into tensor and return it as if it was the video

    # Dataset downloaders
    downloaders:
    - [(obj), src.datasets.panoptic.downloader.PanopticDownloader, res/tmp/panoptic/, res/panoptic_scene_names.txt, true, 23, [-1, 256], null, 8]

    # Datasets
    datasets:
    - [(obj), src.datasets.panoptic.dataset.PanopticDataset, res/tmp/panoptic/, [64, 114], 2, null]
    # - dataset: [(obj), src.datasets.raw_plenoptic_dataset.RawPlenopticDataset, res/tmp/plenoptic/]
    #   shuffle: true
    #   shuffle_before_splitting: true
    #   n_sources: 2
    #   n_targets: null
    #   resize_to: [64, 114]

# Default model configuration
model:
  constructor: [(obj), src.model.DVST]

  # Patch side length (the images will be broken into patches of size p x p)
  p: 16
  # Number of color channels in each frame
  C: 3
  # Number of octaves for representing each of the 6 components from plucker rays and the time component
  # If null, uses raw values instead
  # TODO change code to not use octaves/duplicate coordinates if n_oct = null
  n_oct: 6

  # Number of layers in transformer encoder
  N_enc: 2
  # Number of layers in transformer decoder
  N_dec: 7
  # Dimension of all the vector representations used in the transformer models (dim for all vector inputs in transformers, not just the embedding vectors that will be the latent representations of scenes, idk why did i put such a confusing name but yeah)
  d_model: 192
  # Number of attention heads in both encoder and decoder (n_heads should divide d_model)
  n_heads: 12
  # Expansion factor for mlp blocks after attention blocks in each transformer block (embeddings will be expanded to e_ff * d_model dimensions then contracted back to d_model dimensions)
  e_ff: 4
  qk_norm:
    # Enables QK-Norm
    enabled: true
    # Epsilon for QK-Norm computation
    eps: 1e-4
  # Operation used for attention, should be from xops.fmha
  attn_op: [(obj), xformers.ops.fmha.MemoryEfficientAttentionFlashAttentionOp]

  # Number of embedding vectors used as latent space representation for images
  # TODO could compute this based on frame size and video size
  n_lat: 256
  # The function that will be used to aggregate latents across frames. Should be one of the functions from src.model.latent_aggregators
  latent_aggregator: [(obj), src.model.latent_aggregators.residual_latent_aggregator]
  # TODO How many frames to break scenes into (use if using input scenes that are too big)
  # When using this, after the specified number of frames, the latent_embedding has its grad graph removed and becomes a leaf tensor
  #   Its gradients are not propagated back to the start_latent_embeds parameter, but this saves memory
  scene_batch_size: 6

  # frames_per_scene is the size of the batches that the videos will be broken into to create scenes
  #   TODO this was from the old idea of breaking scene into smaller batches of 3
  #   TODO test 3 ideas separate:
  #     just incremental creation of latents (testing both w or w/o residual)
  #     just breaking into blocks of 3
  #     and test both together (breaking into blocks of size k and doing incremental creation of latents)
  # frames_per_scene: 3

  train:
    # Dropout rate
    # TODO test separate dropouts for each layer
    dropout: 0.1
    loss: [(obj), src.model.loss.PerceptualLoss, null]
    # loss: [(obj), torch.nn.MSELoss, null]

# TODO
# Experiments
# Each experiment has a name and the properties that it will test, specifying for each either a list of possible values to test or "best" to choose the best value from last experiments
# At each experiment, the default model will have the specified properties changed to each of the possible combinations for the given values
# and, if specified, some properties will use the best values found in the last experiments (will error out if no best found before)
experiments:
  - name: Testing number of octaves for representations (n_oct)
    model:
      n_oct: [null, 3, 6, 12]
