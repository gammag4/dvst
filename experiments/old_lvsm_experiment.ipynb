{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe69df74",
   "metadata": {},
   "source": [
    "(tirado do TCC)\n",
    "\n",
    "O raio $\\mathbf{r}_{ij}$ que passa pelo píxel $(i, j)$ é dado por:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbf{r}_{ij} &= (\\mathbf{o}_{ij}, \\mathbf{d}_{ij}) \\\\\n",
    "  \\mathbf{o}_{ij} &= - Q R^{T} \\mathbf{t} \\\\\n",
    "  \\mathbf{d}_{ij} &= \\frac{\\mathbf{d}'_{ij}}{\\lVert \\mathbf{d}'_{ij} \\rVert} \\\\\n",
    "  \\mathbf{d}'_{ij} &= Q R^{T} K^{-1} Q^{-1} \\mathbf{x}_{ij, cam} \\\\\n",
    "  \\mathbf{x}'_{cam} &=\n",
    "  \\begin{bmatrix}\n",
    "    fx + z p_{x} & fy + z p_{y} & z & 1\n",
    "  \\end{bmatrix}^{T}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_compute_plucker_embeddings(f, wx, vecs, T):\n",
    "    # TODO We assume images have even width and height\n",
    "    # Input shapes: (B,), (B,), (B, 3, 3), (B, 3, 4)\n",
    "    # vecs contains the right (vecs[b, 0, :]), up (vecs[b, 1, :]), and forward (vecs[b, 2, :]) unit vectors of the camera in the camera frame\n",
    "    R, t = T[:, :, :3], T[:, :, 3] # Shapes: (B, 3, 3), (B, 3)\n",
    "\n",
    "    # TODO wrong, res_x and res_y should come from image\n",
    "    ry, rx = T.shape[-2], T.shape[-1]\n",
    "    wy = wx * (ry / rx) # Shape (B,)\n",
    "\n",
    "    # Creating tensors with indices\n",
    "    i = torch.arange(rx, dtype=torch.float64, device=T.device)\n",
    "    j = torch.arange(ry, dtype=torch.float64, device=T.device)\n",
    "\n",
    "    # Computing displacements\n",
    "    # Shapes: (W,), (H,)\n",
    "    dx = ((i + 0.5) / rx - 0.5)\n",
    "    dy = -((j + 0.5) / ry - 0.5)\n",
    "\n",
    "    dx2 = torch.einsum('b,i->bi', wx, dx) # dx2_bi = wx_b * dx_i\n",
    "    dy2 = torch.einsum('b,j->bj', wy, dy) # dy2_bj = wy_b * dy_j\n",
    "    \n",
    "    # Computing pixel point in camera frame\n",
    "    v1 = torch.einsum('bi,bc->bic', dx2, vecs[:, 0, :]) # v1_bic = dx2_bi * vr_c\n",
    "    v2 = torch.einsum('bj,bc->bjc', dy2, vecs[:, 1, :]) # v2_bjc = dy2_bj * vu_c\n",
    "    v3 = torch.einsum('b,bc->bc', f, vecs[:, 2, :]) # v3_bc = f_b * vf_c\n",
    "\n",
    "    # q_bijc = v1_bic + v2_bjc + v3_bc\n",
    "    q = v1[:, :, None, :] + v2[:, None, :, :] + v3[:, None, None, :] # TODO test speed with unsqueeze\n",
    "    \n",
    "    p = t[:, :, None, None]\n",
    "    l = torch.einsum('bijc,bkc->bkji', q, R) # l_bijk = q_bijc * R_bkc # TODO test speed with unsqueeze\n",
    "    m = torch.cross(p, l, dim=1)\n",
    "    \n",
    "    # Plucker ray embeddings\n",
    "    pl = torch.cat([l, m], dim=1) # Shape: (B, 6, H, W)\n",
    "\n",
    "    return pl\n",
    "\n",
    "B = 2\n",
    "f = torch.tensor(1, device=device).repeat(B)\n",
    "wx = torch.tensor(8, device=device).repeat(B)\n",
    "vecs = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, -1]], dtype=torch.float64, device=device).repeat(B, 1, 1)\n",
    "t = torch.tensor([[1, 0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=torch.float64, device=device).repeat(B, 1, 1)\n",
    "# img = torch.zeros((B, 3, 4, 4), dtype=torch.float64, device=device)\n",
    "# CreatePluckerRayEmbedding()((f, wx, vecs, T, img))[:, 6:, :, :]\n",
    "compute_plucker_embeddings(f, wx, vecs, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_flatten_embeddings(embeddings, p):\n",
    "    # Input shape: (B, C, H, W)\n",
    "    #TODO use einops\n",
    "    patches = embeddings.unfold(2, p, p).unfold(3, p, p) # Shape: (B, C, H/p, W/p, p, p)\n",
    "    patches = patches.permute(0, 2, 3, 4, 5, 1)\n",
    "    patches = patches.flatten(3, 5).flatten(0, 2) # Shape: (BWH/p^2, Cp^2)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def reverse_patchify_flatten_embeddings(patches, p, W, H):\n",
    "    #TODO use einops\n",
    "    patches = patches.unflatten(0, (-1, H//p, W//p)).unflatten(3, (p, p, -1))\n",
    "    patches = patches.permute(0, 5, 1, 2, 3, 4)\n",
    "    embeddings = patches.permute(0, 1, 2, 4, 3, 5).reshape((*patches.shape[0:2], H, W))\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "x = torch.arange(2*3*16*16).reshape((2, 3, 16, 16))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        assert torch.equal(x.unfold(2, 4, 4).unfold(3, 4, 4)[:, :, i, j, :, :], x[:, :, i*4:i*4+4, j*4:j*4+4])\n",
    "\n",
    "for t in [1, 2, 4, 8, 16]:\n",
    "    assert torch.equal(x, reverse_patchify_flatten_embeddings(patchify_flatten_embeddings(x, t), t, 16, 16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f562e",
   "metadata": {},
   "source": [
    "Perceptual loss (from [LVSM](https://arxiv.org/pdf/2410.17242) paper that says its from\n",
    "[GS-LRM](https://arxiv.org/pdf/2404.19702) paper that says its from\n",
    "[this paper](https://arxiv.org/pdf/1707.09405) which uses a feature reconstruction loss,\n",
    "introduced in [this paper](https://arxiv.org/pdf/1603.08155))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_loss(perceptual_layers, perceptual_params, imgs, target_imgs):\n",
    "    # \n",
    "    # TODO Fix so that the implementation is right (use every layer and use 1 norm instead of 2 and use hyperparameters for each layer)\n",
    "    x1, x2 = imgs, target_imgs\n",
    "    losses = []\n",
    "\n",
    "    for l in perceptual_layers:\n",
    "        x1, x2 = l(x1), l(x2)\n",
    "        C, H, W = x1.shape[-3], x1.shape[-2], x1.shape[-1]\n",
    "        losses.append(torch.norm(x1 - x2, p=2, dim=-1).sum() / (C * H * W))\n",
    "\n",
    "    print(losses)\n",
    "    res = (torch.concat(losses, device=imgs.device) *  perceptual_params).sum()\n",
    "    # res = torch.concat(losses, device=imgs.device).sum()\n",
    "    return res\n",
    "\n",
    "perceptual_model = torchvision.models.convnext_tiny(torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "perceptual_model\n",
    "perceptual_model = perceptual_model.features\n",
    "perceptual_layers = [lambda x: x] + list(perceptual_model)\n",
    "perceptual_params = torch.ones(8, dtype=torch.float64)\n",
    "shape = (4, 3, 64, 64)\n",
    "I = torch.rand(shape)\n",
    "[\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, I, I),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.zeros(shape), torch.ones(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.ones(shape), torch.zeros(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.zeros(shape), torch.zeros(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.ones(shape), torch.ones(shape)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57859b9",
   "metadata": {},
   "source": [
    "TODO document these in obsidian\n",
    "\n",
    "TODO where to send data to device: send it when loading from dataloader\n",
    "\n",
    "```py\n",
    "for x_data, y_data in train_dataloader:\n",
    "    x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "```\n",
    "\n",
    "check\n",
    "pytorch lightning dataloader for device\n",
    "https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
    "    LightningDataModule.transfer_batch_to_device()\n",
    "https://github.com/Lightning-AI/pytorch-lightning/issues/3341\n",
    "\n",
    "TODO how does it compute gradients for a batch? it keeps aggregating the gradient for individual elements of the batch until we use step and zero_grad, then a new batch starts\n",
    "\n",
    "TODO\n",
    "- first layer creates batch tensor with multiple source and target images and a second tensor with indices of target images in first tensor\n",
    "- second layer (plucker embeddings) receives only batch tensor and outputs plucker ray embeddings\n",
    "- third layer projects embeddings onto linear tokens\n",
    "- fourth layer is transformer\n",
    "- then only in fifth layer we use the target images' indices tensor to get resulting images and compare to actual target images\n",
    "\n",
    "TODO check tensor views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVSM(nn.Module):\n",
    "    def __init__(self, p, d, l, C, N, h):\n",
    "        # p is patch size, d is latent size, l is number of latent tokens, C is number of channels in each image\n",
    "        # N is number of encoder/decoder layers, h is number of attention heads\n",
    "        super().__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.l = l\n",
    "        self.C = C\n",
    "        \n",
    "        self.linear_in = nn.Linear(in_features=(C + 6) * p * p, out_features=d)\n",
    "        self.target_linear = nn.Linear(in_features=6 * p * p, out_features=d)\n",
    "        self.layer_out = nn.Sequential([\n",
    "            nn.Linear(in_features=d, out_features=(C + 6) * p * p),\n",
    "            nn.Sigmoid(),\n",
    "        ])\n",
    "        \n",
    "        xformer_config = [\n",
    "            {\n",
    "                \"reversible\": False,\n",
    "                \"block_type\": \"encoder\",\n",
    "                \"num_layers\": N,\n",
    "                \"dim_model\": d,\n",
    "                \"residual_norm_style\": \"pre\",\n",
    "                # \"position_encoding_config\": {\n",
    "                #     \"name\": \"sine\",\n",
    "                #     \"seq_len\": self.hparams.block_size,\n",
    "                # },\n",
    "                \"multi_head_config\": {\n",
    "                    \"num_heads\": h,\n",
    "                    \"residual_dropout\": 0.1,\n",
    "                    \"use_rotary_embeddings\": False,\n",
    "                    \"attention\": {\n",
    "                        \"name\": self.hparams.attention,\n",
    "                        \"dropout\": 0.1,\n",
    "                        \"causal\": False,\n",
    "                        \"seq_len\": self.hparams.block_size,\n",
    "                        \"num_rules\": self.hparams.n_head,\n",
    "                    },\n",
    "                },\n",
    "                \"feedforward_config\": {\n",
    "                    \"name\": \"MLP\",\n",
    "                    \"dropout\": 0.1,\n",
    "                    \"activation\": \"gelu\",\n",
    "                    \"hidden_layer_multiplier\": self.hparams.hidden_layer_multiplier,\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        config = xFormerConfig(xformer_config)\n",
    "        config.weight_init = 'small'\n",
    "        self.model = xFormer.from_config(config)\n",
    "        \n",
    "    def forward(self, source_batch, target_batch):\n",
    "        # TODO We assume p divides W and H\n",
    "        # TODO Here we compute the plucker embeddings in the whole image before breaking it into patches, but in the paper they break the image first, then compute the patches later (test patching before and after computing plucker ray embeddings)\n",
    "        # TODO we assume batch size 1, so B will be the number of input images of that batch\n",
    "        # Shapes: (B,), (B,), (B, 3, 3), (B, 3, 4), (B, C, H, W)\n",
    "        f, wx, vecs, T, imgs = source_batch\n",
    "        f2, wx2, vecs2, T2 = target_batch\n",
    "        W, H = imgs.shape[3], imgs.shape[2]\n",
    "\n",
    "        # Sources and target plucker rays\n",
    "        # Shapes: (B, C + 6, H, W), (1, 6, H, W)\n",
    "        source_pl = compute_plucker_embeddings(f, wx, vecs, T)\n",
    "        source_pl = torch.cat([imgs, source_pl], dim=-3)\n",
    "        target_pl = compute_plucker_embeddings(f2, wx2, vecs2, T2)\n",
    "        \n",
    "        # Creates and flattens patches\n",
    "        source_flattened_patches = patchify_flatten_embeddings(source_pl, self.p)\n",
    "        target_flattened_patches = patchify_flatten_embeddings(target_pl, self.p)\n",
    "\n",
    "        # Linear transformation so that they have same shape\n",
    "        # TODO test directly sending them flattened instead of using linear. in that case, the target patches would need images, but these would be either zeros or learned latent tokens\n",
    "        source_tokens = self.linear_in(source_flattened_patches)\n",
    "        target_tokens = self.target_linear(target_flattened_patches)\n",
    "        \n",
    "        # Generates output\n",
    "        # TODO use inheritance use abstract function here and specialize into enc-dec and dec-only\n",
    "        output = self.model(torch.cat([source_tokens, target_tokens]))\n",
    "        \n",
    "        # Generates target embeddings back from output\n",
    "        # Source outputs are discarded\n",
    "        target_tokens_out = output[source_tokens.shape[0]:]\n",
    "        target_flattened_patches_out = self.layer_out(target_tokens_out)\n",
    "        target_embs_out = reverse_patchify_flatten_embeddings(target_flattened_patches_out, self.p, W, H)\n",
    "        \n",
    "        # TODO test using linear instead of just stripping off the final plucker embeddings from the tokens\n",
    "        out_imgs = target_embs_out[:, :-6, :, :]\n",
    "        return out_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e79d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "a(torch.randn(14, 7, 10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(L.LightningModule):\n",
    "    def __init__(self, perceptual_params):\n",
    "        super().__init__()\n",
    "        # perceptual_params is the parameters for the weighted perceptual loss of the rendered images\n",
    "        \n",
    "        self.lvsm = LVSM() #TODO\n",
    "\n",
    "        # Perceptual loss layers\n",
    "        # self.perceptual_layers = torchvision.models.vgg19(torchvision.models.VGG19_Weights.DEFAULT).features\n",
    "        perceptual_model = torchvision.models.convnext_tiny(torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "        perceptual_model.eval()\n",
    "        self.perceptual_layers = perceptual_model.features\n",
    "        self.perceptual_layers = [lambda x: x] + list(self.perceptual_layers) # This makes the first layer the identity (this layer is used to compute the MSE loss)\n",
    "        self.perceptual_params = torch.tensor(perceptual_params)\n",
    "\n",
    "    def step(self, batch):\n",
    "        f, wx, vecs, T, imgs = batch\n",
    "\n",
    "        # For each batch, we choose the last image as the target image\n",
    "        # TODO create multiple targets instead of a single one\n",
    "        # TODO to do this, probably we would pass the source tokens and copy the transformer state at that point and create target tokens for each image from there\n",
    "        source_batch = f[:-1], wx[:-1], vecs[:-1], T[:-1], imgs[:-1]\n",
    "        target_batch = f[-1:], wx[-1:], vecs[-1:], T[-1:]\n",
    "        target_imgs = imgs[-1:]\n",
    "        \n",
    "        gen_imgs = self.lvsm(source_batch, target_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f78bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvsm = LVSM(16, 20, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a316e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetStandardizer(nn.Module):\n",
    "    # axis_permutation shape (3, 3) converts permuted/negative axes to <TODO standard>\n",
    "    # intrinsic_01_range whether the intrinsics map to the 0-1 range\n",
    "    # time_scaling = 1/FPS\n",
    "    def __init__(self, axis_permutation, intrinsic_01_range, time_scaling):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, I, E, K, t):\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
