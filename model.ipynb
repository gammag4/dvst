{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import progressbar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "from open3d.visualization import draw_plotly\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import einops\n",
    "import einx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.amp as amp\n",
    "import torch.nn.utils as utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler, SubsetRandomSampler, BatchSampler\n",
    "import torchvision\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import save_image\n",
    "from torchinfo import summary\n",
    "from torchcodec.decoders import VideoDecoder\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.panoptic.dataset import PanopticDataset\n",
    "from src.datasets.raw_plenoptic_dataset import RawPlenopticDataset\n",
    "from src.datasets.full_dataset import FullDataset\n",
    "\n",
    "from src.model.pose_encoder import compute_pad, compute_octaves, compute_view_rays\n",
    "\n",
    "from src.config import load_config\n",
    "\n",
    "from src.model import PoseEncoder, DVST, latent_aggregators\n",
    "\n",
    "from src.draw import get_camera_geometry\n",
    "\n",
    "from src.utils import get_video_slice, preprocess_scene_videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cu126'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVST Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, torch.bfloat16, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make it easier to pass around and validate configs\n",
    "config = load_config('res/config.yaml')\n",
    "\n",
    "config.setup.ddp.rank, config.setup.amp.dtype, config.setup.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full = FullDataset(config.train.data.datasets)\n",
    "len(dataset_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DVST(config=config.model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 7.87M; Trainable params: 7.87M\n"
     ]
    }
   ],
   "source": [
    "from src.utils import get_num_params\n",
    "\n",
    "get_num_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc110>,\n",
       "   'K': tensor([[1.6547e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.5388e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 6.0433e-04,  0.0000e+00, -5.8016e-01],\n",
       "           [ 0.0000e+00,  6.4986e-04, -3.5093e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.5296, -0.0115,  0.8482],\n",
       "            [ 0.6366,  0.6554,  0.4064],\n",
       "            [-0.5606,  0.7552, -0.3397]]], device='cuda:0'),\n",
       "   't': tensor([[ -5.6521,  81.6465, 378.2934]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc080>,\n",
       "   'K': tensor([[1.4083e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3423e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 7.1006e-04,  0.0000e+00, -6.8166e-01],\n",
       "           [ 0.0000e+00,  7.4501e-04, -4.0230e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[ 0.0536,  0.0240,  0.9983],\n",
       "            [ 0.6374,  0.7687, -0.0527],\n",
       "            [-0.7687,  0.6391,  0.0259]]], device='cuda:0'),\n",
       "   't': tensor([[  6.6445, 105.7245, 361.4694]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])}],\n",
       " 'queries': [{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fdb20032d80>,\n",
       "   'K': tensor([[1.4302e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3539e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 6.9920e-04,  0.0000e+00, -6.7124e-01],\n",
       "           [ 0.0000e+00,  7.3858e-04, -3.9883e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.9970,  0.0545, -0.0555],\n",
       "            [-0.0107,  0.6108,  0.7917],\n",
       "            [ 0.0771,  0.7899, -0.6084]]], device='cuda:0'),\n",
       "   't': tensor([[-17.6922,  64.0220, 380.7844]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fdb20033410>,\n",
       "   'K': tensor([[1.4118e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3716e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 7.0833e-04,  0.0000e+00, -6.8000e-01],\n",
       "           [ 0.0000e+00,  7.2906e-04, -3.9369e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.9415, -0.0377,  0.3348],\n",
       "            [ 0.1807,  0.7824,  0.5960],\n",
       "            [-0.2844,  0.6217, -0.7298]]], device='cuda:0'),\n",
       "   't': tensor([[ -7.3450, 113.0683, 359.1882]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd936378380>,\n",
       "   'K': tensor([[1.4097e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3722e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 7.0936e-04,  0.0000e+00, -6.8099e-01],\n",
       "           [ 0.0000e+00,  7.2874e-04, -3.9352e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[ 0.9448, -0.0489, -0.3241],\n",
       "            [-0.2141,  0.6567, -0.7231],\n",
       "            [ 0.2482,  0.7526,  0.6100]]], device='cuda:0'),\n",
       "   't': tensor([[ -8.3128,  83.8591, 377.3930]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9363fe360>,\n",
       "   'K': tensor([[1.4107e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3299e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 7.0888e-04,  0.0000e+00, -6.8053e-01],\n",
       "           [ 0.0000e+00,  7.5194e-04, -4.0605e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.6212, -0.0284,  0.7832],\n",
       "            [ 0.0751,  0.9926,  0.0955],\n",
       "            [-0.7801,  0.1182, -0.6144]]], device='cuda:0'),\n",
       "   't': tensor([[-15.3971, 117.3840, 288.2436]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9362ffdd0>,\n",
       "   'K': tensor([[1.4357e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3433e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 6.9655e-04,  0.0000e+00, -6.6869e-01],\n",
       "           [ 0.0000e+00,  7.4443e-04, -4.0199e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.9317, -0.0109, -0.3631],\n",
       "            [-0.0217,  0.9994,  0.0257],\n",
       "            [ 0.3626,  0.0318, -0.9314]]], device='cuda:0'),\n",
       "   't': tensor([[ -8.1925, 143.5502, 280.5694]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9363bfb90>,\n",
       "   'K': tensor([[1.6629e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.5779e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 6.0134e-04,  0.0000e+00, -5.7729e-01],\n",
       "           [ 0.0000e+00,  6.3377e-04, -3.4223e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[ 0.1356,  0.0332, -0.9902],\n",
       "            [-0.0915,  0.9956,  0.0208],\n",
       "            [ 0.9865,  0.0878,  0.1381]]], device='cuda:0'),\n",
       "   't': tensor([[ 20.3223, 126.5647, 284.8756]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc110>,\n",
       "   'K': tensor([[1.6547e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.5388e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 6.0433e-04,  0.0000e+00, -5.8016e-01],\n",
       "           [ 0.0000e+00,  6.4986e-04, -3.5093e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[-0.5296, -0.0115,  0.8482],\n",
       "            [ 0.6366,  0.6554,  0.4064],\n",
       "            [-0.5606,  0.7552, -0.3397]]], device='cuda:0'),\n",
       "   't': tensor([[ -5.6521,  81.6465, 378.2934]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])},\n",
       "  {'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc080>,\n",
       "   'K': tensor([[1.4083e+03, 0.0000e+00, 9.6000e+02],\n",
       "           [0.0000e+00, 1.3423e+03, 5.4000e+02],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0000e+00]], device='cuda:0'),\n",
       "   'Kinv': tensor([[ 7.1006e-04,  0.0000e+00, -6.8166e-01],\n",
       "           [ 0.0000e+00,  7.4501e-04, -4.0230e-01],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0'),\n",
       "   'R': tensor([[[ 0.0536,  0.0240,  0.9983],\n",
       "            [ 0.6374,  0.7687, -0.0527],\n",
       "            [-0.7687,  0.6391,  0.0259]]], device='cuda:0'),\n",
       "   't': tensor([[  6.6445, 105.7245, 361.4694]], device='cuda:0'),\n",
       "   'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "           2.0260e+02], device='cuda:0'),\n",
       "   'shape': torch.Size([6073, 3, 1080, 1920])}],\n",
       " 'targets': [<torchcodec.decoders._video_decoder.VideoDecoder at 0x7fdb20032d80>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fdb20033410>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd936378380>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9363fe360>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9362ffdd0>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9363bfb90>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc110>,\n",
       "  <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fd9360fc080>],\n",
       " 'n_frames': 6073}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = preprocess_scene_videos(dataset_full[0], device)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 476.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 478.81 MiB is free. Including non-PyTorch memory, this process has 5.74 GiB memory in use. Of the allocated memory 4.79 GiB is allocated by PyTorch, and 30.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     i\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize((s\u001b[38;5;241m.\u001b[39mn_frames, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s\u001b[38;5;241m.\u001b[39mtargets)):\n\u001b[0;32m---> 10\u001b[0m     s\u001b[38;5;241m.\u001b[39mtargets[i] \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m[:, :, :\u001b[38;5;241m64\u001b[39m, :\u001b[38;5;241m64\u001b[39m]\n\u001b[1;32m     12\u001b[0m s\u001b[38;5;241m.\u001b[39msources[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Projects/tcc files/dvst/src/utils.py:112\u001b[0m, in \u001b[0;36mget_video_slice\u001b[0;34m(v, start, end)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_video_slice\u001b[39m(v, start, end):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01min\u001b[39;00m [VideoDecoder, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\n\u001b[1;32m    114\u001b[0m     res \u001b[38;5;241m=\u001b[39m edict(\n\u001b[1;32m    115\u001b[0m         K\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mK,\n\u001b[1;32m    116\u001b[0m         Kinv\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mKinv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         shape\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mSize([end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m*\u001b[39mv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]])\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 476.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 478.81 MiB is free. Including non-PyTorch memory, this process has 5.74 GiB memory in use. Of the allocated memory 4.79 GiB is allocated by PyTorch, and 30.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "s.n_frames = 20\n",
    "\n",
    "for i in s.sources:\n",
    "    i.video = get_video_slice(i.video, 0, s.n_frames)[:, :, :64, :64]\n",
    "    i.shape = torch.Size((s.n_frames, 3, 64, 64))\n",
    "for i in s.queries:\n",
    "    i.video = get_video_slice(i.video, 0, s.n_frames)[:, :, :64, :64]\n",
    "    i.shape = torch.Size((s.n_frames, 3, 64, 64))\n",
    "for i in range(len(s.targets)):\n",
    "    s.targets[i] = get_video_slice(s.targets[i], 0, s.n_frames)[:, :, :64, :64]\n",
    "\n",
    "s.sources[0].video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DVST.forward() missing 4 required positional arguments: 'targets', 'start', 'end', and 'latent_embeds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m----> 2\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: DVST.forward() missing 4 required positional arguments: 'targets', 'start', 'end', and 'latent_embeds'"
     ]
    }
   ],
   "source": [
    "with amp.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    out = model(s, lambda loss: print(loss) or loss.backward())\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# configure transformer enc and dec layers\n",
    "# add optimizations checkpointing mixed precision etc\n",
    "# do first testing of model w small parameters and check how much the pc can handle of it\n",
    "# create combinations of configs for small experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
