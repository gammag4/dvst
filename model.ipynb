{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO lazy hf dataset\n",
    "\n",
    "# https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable\n",
    "#  search for yield\n",
    "#  use this to lazily load the videos (in each iteration download the next one and return the current (that has already been downloaded or download too if not))\n",
    "# https://huggingface.co/docs/datasets/en/video_load\n",
    "# https://huggingface.co/docs/datasets/en/video_dataset\n",
    "#  create video dataset\n",
    "# https://huggingface.co/docs/datasets/en/about_map_batch\n",
    "#  use to map transformations (resizing etc)\n",
    "# https://github.com/iejMac/video2dataset\n",
    "#  check to see how to paralelize the yield (and how to create it abstractly for any dataset of scenes with a list of videos, not just panoptic)\n",
    "#  actually i think i can do that just using dataset.map batched + yield and dataset.take in streaming dataset, but you would need\n",
    "\n",
    "# make a dataset that creates a uniform distribution of different video sizes/aspect ratios/cropping options\n",
    "# then evaluate the model in these environments:\n",
    "#  same size/aspect/cropping on entire dataset\n",
    "#  same size/aspect/cropping for videos in a scene but varying for all scenes\n",
    "#  varying size/aspect/cropping for all videos in all scenes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import progressbar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "from open3d.visualization import draw_plotly\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import einops\n",
    "import einx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler, SubsetRandomSampler, BatchSampler\n",
    "import torchvision\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import save_image\n",
    "from torchinfo import summary\n",
    "from torchcodec.decoders import VideoDecoder\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as callbacks\n",
    "import xformers\n",
    "# from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.panoptic_dataset import PanopticDataset\n",
    "from src.plenoptic_dataset import PlenopticDataset\n",
    "\n",
    "from src.draw import get_camera_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cu126'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panoptic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_panoptic = PanopticDataset('res/tmp/panoptic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fa810e62e40>,\n",
       " 'K': tensor([[1.4107e+03, 0.0000e+00, 9.6000e+02],\n",
       "         [0.0000e+00, 1.3299e+03, 5.4000e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]),\n",
       " 'Kinv': tensor([[ 7.0888e-04,  0.0000e+00, -6.8053e-01],\n",
       "         [ 0.0000e+00,  7.5194e-04, -4.0605e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]),\n",
       " 'R': tensor([[[-0.6212, -0.0284,  0.7832],\n",
       "          [ 0.0751,  0.9926,  0.0955],\n",
       "          [-0.7801,  0.1182, -0.6144]]]),\n",
       " 't': tensor([[-15.3971, 117.3840, 288.2436]]),\n",
       " 'time': tensor([0.0000e+00, 3.3367e-02, 6.6733e-02,  ..., 2.0254e+02, 2.0257e+02,\n",
       "         2.0260e+02]),\n",
       " 'shape': [6073, 3, 1080, 1920]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = dataset_panoptic.__getitem__(0)\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchcodec.decoders._video_decoder.VideoDecoder at 0x7fa810e62e40>,\n",
       " tensor([[1.4107e+03, 0.0000e+00, 9.6000e+02],\n",
       "         [0.0000e+00, 1.3299e+03, 5.4000e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]),\n",
       " tensor([[[-0.6212, -0.0284,  0.7832],\n",
       "          [ 0.0751,  0.9926,  0.0955],\n",
       "          [-0.7801,  0.1182, -0.6144]]]),\n",
       " tensor([[-15.3971, 117.3840, 288.2436]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, K, R, t2 = [v[0][i] for i in ['video', 'K', 'R', 't']]\n",
    "v, K, R, t2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plenoptic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_plenoptic = PlenopticDataset('res/tmp/plenoptic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7fa810e62f00>,\n",
       " 'K': tensor([[1.4585e+03, 0.0000e+00, 1.3520e+03],\n",
       "         [0.0000e+00, 1.4585e+03, 1.0140e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]),\n",
       " 'Kinv': tensor([[ 6.8564e-04,  0.0000e+00, -9.2698e-01],\n",
       "         [ 0.0000e+00,  6.8564e-04, -6.9523e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  1.0000e+00]]),\n",
       " 'R': tensor([[[-0.0272,  0.8776,  0.4786],\n",
       "          [ 0.9996,  0.0286,  0.0042],\n",
       "          [-0.0100,  0.4786, -0.8780]]], dtype=torch.float64),\n",
       " 't': tensor([[ 5.4591, -1.0853,  0.6145]], dtype=torch.float64),\n",
       " 'time': tensor([0.0000e+00, 3.3333e-02, 6.6667e-02,  ..., 3.9900e+01, 3.9933e+01,\n",
       "         3.9967e+01]),\n",
       " 'shape': [1200, 3, 2028, 2704]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = dataset_plenoptic.__getitem__(0)\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 4], (0, 0, 1, 2))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_pad(hw, p):\n",
    "    # Pads the input so that it is divisible by 'p'\n",
    "    # hw: (2,), p: (1)\n",
    "    \n",
    "    pad_raw = [((p - i) % p) for i in hw]\n",
    "    pad_s = [i // 2 for i in pad_raw]\n",
    "    pad = (pad_s[1], pad_raw[1] - pad_s[1], pad_s[0], pad_raw[0] - pad_s[0])\n",
    "    hw_padded = [i + d for i, d in zip(hw, pad_raw)]\n",
    "    \n",
    "    # pad: (pad_width_start, pad_width_end, pad_height_start, pad_height_end) (starts from last dimension to pad)\n",
    "    # hw_padded: (2,), pad: (4,)\n",
    "    return hw_padded, pad\n",
    "\n",
    "compute_pad([5, 4], 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_view_rays(vecs, Kinv, R, t):\n",
    "    # Computes view rays (o, d)\n",
    "    # vecs: meshgrid vecs, first dim is (x, y, z)\n",
    "    # vecs: (3, h, w), Kinv, R: (B, 3, 3), t: (B, 3)\n",
    "\n",
    "    # TODO check without double precision\n",
    "    vecs, Kinv, R, t = [i.to(torch.float64) for i in (vecs, Kinv, R, t)]\n",
    "\n",
    "    h, w = vecs.shape[-2:]\n",
    "\n",
    "    o = -einx.dot('... h w, ... h -> ... w', R, t)  # -R^T t\n",
    "    o = einx.rearrange('... c -> ... c h w', o, h=h, w=w) # repeat o for each vec # TODO repeating maybe not needed\n",
    "    d = einx.dot('... x1 c2, x1 c, c h w -> ... c2 h w', R.to(torch.float64), Kinv.to(torch.float64), vecs) # R^T K^-1 x_ij,cam # TODO check without double precision\n",
    "    d = d / einx.sum('b [c] h w -> b 3 h w', d * d).sqrt() # normalize d\n",
    "\n",
    "    # o, d: (B, 3, H, W)\n",
    "    return o, d\n",
    "\n",
    "def compute_plucker_rays(o, d):\n",
    "    # o, d: (B, 3, H, W)\n",
    "\n",
    "    l = torch.cross(o, d, dim=-3)\n",
    "    rays = torch.concat([d, l], dim=-3)\n",
    "\n",
    "    # rays: (B, 6, H, W)\n",
    "    return rays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.7423e-08,  0.0000e+00],\n",
       "         [-1.0000e+00,  1.0000e+00],\n",
       "         [ 1.7485e-07,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 3.4969e-07,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 6.9938e-07,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 1.0000e+00,  1.0000e+00]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_octaves(v, n_oct, dim=-1):\n",
    "    assert dim < 0, 'No positive dim allowed'\n",
    "\n",
    "    v = v * torch.pi\n",
    "    tensors = [torch.sin(v), torch.cos(v)]\n",
    "    last = v\n",
    "    for _ in range(n_oct - 1):\n",
    "        last = last * 2\n",
    "        tensors.append(torch.sin(last))\n",
    "        tensors.append(torch.cos(last))\n",
    "        \n",
    "    \n",
    "\n",
    "    return torch.stack(tensors, dim=dim).flatten(dim - 1, dim)\n",
    "\n",
    "v = torch.zeros((3, 6, 2))\n",
    "v[0, 0, 0] = 1\n",
    "compute_octaves(v, n_oct=4, dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PoseEncoder(nn.Module):\n",
    "    def __init__(self, d_lat, n_oct, C, p):\n",
    "        super().__init__()\n",
    "        self.d_lat = d_lat\n",
    "        self.n_oct = n_oct\n",
    "        self.C = C\n",
    "        self.p = p\n",
    "\n",
    "        # TODO initialize w gaussian\n",
    "        # (C, p, p)\n",
    "        self.im_parameter = nn.Parameter(torch.zeros((self.C, self.p, self.p)))\n",
    "\n",
    "        # TODO check without double precision\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=(12 * self.n_oct + self.C) * self.p ** 2 + 2 * self.n_oct,\n",
    "            #in_features=(6 + self.C) * self.p ** 2 + 1, # Without octaves, just for testing\n",
    "            out_features=d_lat,\n",
    "            dtype=torch.float64\n",
    "        )\n",
    "        \n",
    "    def _compute_view_rays(self, Kinv, R, t, pad, hw_padded):\n",
    "        # The forward function was split into two to display the view rays layer\n",
    "        \n",
    "        pad_s = pad[-2::-2]\n",
    "\n",
    "        # Creates vectors for each pixel in screen\n",
    "        # No need to unflip y axis since it being flipped does not affect the topological structure of the representation TODO is it true?\n",
    "        ranges = [torch.arange(l, dtype=torch.float64) - o + 0.5 for o, l in zip(pad_s, hw_padded)]\n",
    "        # In the original LVSM impl, the K^{-1} multiplication is done here bc its faster, maybe change the code to do that too (https://github.com/Haian-Jin/LVSM/blob/ebeff4989a3e1ec38fcd51ae24919d0eadf38c8f/utils/data_utils.py#L71-L73)\n",
    "        # Used torch.ones since it seems to be used by most of the vision models similar to this (e.g. lvsm, see https://github.com/Haian-Jin/LVSM/blob/ebeff4989a3e1ec38fcd51ae24919d0eadf38c8f/utils/data_utils.py#L73)\n",
    "        vecs = torch.meshgrid(*ranges, indexing='ij')\n",
    "        vecs = torch.concat([torch.stack([*vecs[::-1]]), torch.ones((1, *vecs[0].shape))], dim=-3)\n",
    "\n",
    "        o, d = compute_view_rays(vecs, Kinv, R, t) # o, d: (B, 3, H, W)\n",
    "        return o, d\n",
    "\n",
    "    # I = images, HW = tuple with height and width\n",
    "    # Set both if image has been resized, specifying original image height and width in HW\n",
    "    # We assume images are already resized (always resize them maintaining aspect ratio)\n",
    "    # We assume images are already padded so that p divides H and W\n",
    "    # We assume that the K matrix uses xy mapping instead of uv (sensor area is real in range [(0, 0), (h, w)], not [(0, 0), (1, 1)])\n",
    "    # We assume images are in type float with colors in range 0-1\n",
    "    def forward(self, Kinv, R, t, time, I=None, hw=None):\n",
    "        # I: (B, C, H, W), K, Kinv, R: (B, 3, 3), t: (B, 3), time: (B,), hw: (2,)\n",
    " \n",
    "        #TODO corrige hw\n",
    "        #TODO tem que retornar quanto de padding teve pra tirar o padding na comparacao da loss function\n",
    "        #TODO na verdade no lugar de retornar o padding ja retorna a visao prevista com padding retirado no modelo final\n",
    "        \n",
    "        assert (I == None) ^ (hw == None), 'Either I or HW or both should be set'\n",
    "        \n",
    "        if I is not None:\n",
    "            hw = I.shape[-2:]\n",
    "            I = I * 2 - 1 # Normalizing image\n",
    "\n",
    "        # Pads the input so that it is divisible by 'p'\n",
    "        hw_padded, pad = compute_pad(hw, self.p)\n",
    "        I = F.pad(I, pad, 'constant', 0) if I is not None else None\n",
    "\n",
    "        o, d = self._compute_view_rays(Kinv, R, t, pad, hw_padded)\n",
    "        plucker_rays = compute_plucker_rays(o, d) # (B, 6, H, W)\n",
    "\n",
    "        # (B, 2 * 6 * n_oct, H, W)\n",
    "        plucker_octs = compute_octaves(plucker_rays, self.n_oct, dim=-3)\n",
    "        #plucker_octs = torch.concat([plucker_octs, I * 2 - 1], dim=-3) if I is not None else plucker_octs # Transforming and concatenating image\n",
    "\n",
    "        # Concatenating image with octaves and rearranging into patches\n",
    "        # (B, HW/p^2, (12 * n_oct + C) * p^2)\n",
    "        if I is None:\n",
    "            patches = einx.rearrange('... c1 (h p1) (w p2), c2 p1 p2 -> ... (h w) ((c1 + c2) p1 p2)', plucker_octs, self.im_parameter, p1=self.p, p2=self.p)\n",
    "        else:\n",
    "            patches = einx.rearrange('... c1 (h p1) (w p2), ... c2 (h p1) (w p2) -> ... (h w) ((c1 + c2) p1 p2)', plucker_octs, I, p1=self.p, p2=self.p)\n",
    "\n",
    "        time_octs = compute_octaves(time.unsqueeze(-1), self.n_oct, dim=-1) # (B, 2 * n_oct)\n",
    "\n",
    "        # (B, HW/p^2, (12 * n_oct + C) * p^2 + 2 * n_oct)\n",
    "        tokens = einx.rearrange('... hw c1, ... c2 -> ... hw (c1 + c2)', patches, time_octs)\n",
    "        tokens = self.linear(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "B = 4\n",
    "C = 2\n",
    "K = torch.linalg.inv(torch.arange(9).reshape((3, 3)) + 4.0)\n",
    "Kinv = K.inverse()\n",
    "R, t = torch.arange(B * 9).reshape((B, 3, 3)), torch.arange(B * 3).reshape((B, 3))\n",
    "I = torch.ones((B, C, 5, 4))\n",
    "\n",
    "pose_encoder = PoseEncoder(d_lat=12, n_oct=6, C=C, p=4)\n",
    "pose_encoder.forward(Kinv, R, t, torch.arange(B) / 4, I).shape # (4, 2, 12)\n",
    "#a.forward(Kinv, R, t, torch.arange(B) / 4, None, I.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to RawDVST, create DVST that also has CNN to reduce dims and PoseWrapper to add a pose estimator to both\n",
    "class DVST(nn.Module):\n",
    "    # not specified: H, W, C, N_{context}\n",
    "    # n_heads has to divide d_lat\n",
    "    # p has to divide H and W (padding, cropping and resizing)\n",
    "    def __init__(self, N_enc, N_dec, n_heads, d_lat, e_ff, n_lat, p, n_oct):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_lat % n_heads == 0, \"n_heads should divide d_lat\"\n",
    "\n",
    "        self.N_enc = N_enc\n",
    "        self.N_dec = N_dec\n",
    "        self.n_heads = n_heads\n",
    "        self.d_lat = d_lat\n",
    "        self.e_ff = e_ff\n",
    "        self.n_lat = n_lat\n",
    "        self.p = p\n",
    "        self.n_oct = n_oct\n",
    "        \n",
    "        self.pose_encoder = PoseEncoder(self.d_lat, self.n_oct, self.C, self.p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Kinv, R, t, time, I, hw\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
