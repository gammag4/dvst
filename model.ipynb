{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import random\n",
    "import progressbar\n",
    "\n",
    "import einops\n",
    "import einx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as utils\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler, RandomSampler, SubsetRandomSampler, BatchSampler\n",
    "import torchvision\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import save_image\n",
    "from torchinfo import summary\n",
    "from torchcodec.decoders import VideoDecoder\n",
    "import lightning as L\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.callbacks as callbacks\n",
    "import xformers\n",
    "# from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.panoptic_downloader import PanopticDownloader, PanopticScene\n",
    "from src.panoptic_dataset import PanopticDataset\n",
    "from src.plenoptic_dataset import PlenopticDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cu126'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = PanopticDownloader(device='cuda')\n",
    "# # d._get_scene_names = lambda _: ['161029_hands2']  # TODO\n",
    "# await d.load(scene_names_file='res/tmp/panoptic_scene_names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.download_views('res/tmp/scenes/', 'hd', 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = PanopticDataset('res/tmp/scenes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = d.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchcodec.decoders._video_decoder.VideoDecoder at 0x7efeea2b0440>,\n",
       " [11598, 3, 1080, 1920])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]['video'], v[0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7efeea2b0440>,\n",
       " 'K': [[1396.07, 0, 949.125], [0, 1392.72, 548.555], [0, 0, 1]],\n",
       " 'R': [[-0.9413327602, -0.03771956774, 0.3353652766],\n",
       "  [0.1810483745, 0.782187724, 0.5961575719],\n",
       "  [-0.2848054083, 0.6218999909, -0.7294698628]],\n",
       " 't': [[-6.467249592], [112.8077413], [359.5437064]],\n",
       " 'fps': 29.97,\n",
       " 'shape': [11598, 3, 1080, 1920]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchcodec.decoders._video_decoder.VideoDecoder at 0x7efeea2b0440>,\n",
       " [[1396.07, 0, 949.125], [0, 1392.72, 548.555], [0, 0, 1]],\n",
       " [[-0.9413327602, -0.03771956774, 0.3353652766],\n",
       "  [0.1810483745, 0.782187724, 0.5961575719],\n",
       "  [-0.2848054083, 0.6218999909, -0.7294698628]],\n",
       " [[-6.467249592], [112.8077413], [359.5437064]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, K, R, t = [v[0][i] for i in ['video', 'K', 'R', 't']]\n",
    "v, K, R, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = PlenopticDataset('res/tmp/plenoptic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.plenoptic_dataset.PlenopticDataset at 0x7efeea2b2000>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = d2.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': <torchcodec.decoders._video_decoder.VideoDecoder at 0x7f0040d1e840>,\n",
       " 'K': tensor([[1.4585e+03, 0.0000e+00, 1.3520e+03],\n",
       "         [0.0000e+00, 1.4585e+03, 1.0140e+03],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00]]),\n",
       " 'R': tensor([[-0.0272,  0.8776,  0.4786],\n",
       "         [ 0.9996,  0.0286,  0.0042],\n",
       "         [-0.0100,  0.4786, -0.8780]], dtype=torch.float64),\n",
       " 't': tensor([[ 5.4591],\n",
       "         [-1.0853],\n",
       "         [ 0.6145]], dtype=torch.float64),\n",
       " 'fps': 60,\n",
       " 'shape': [1200, 3, 2028, 2704]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tirado do TCC)\n",
    "\n",
    "O raio $\\mathbf{r}_{ij}$ que passa pelo píxel $(i, j)$ é dado por:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbf{r}_{ij} &= (\\mathbf{o}_{ij}, \\mathbf{d}_{ij}) \\\\\n",
    "  \\mathbf{o}_{ij} &= - Q R^{T} \\mathbf{t} \\\\\n",
    "  \\mathbf{d}_{ij} &= \\frac{\\mathbf{d}'_{ij}}{\\lVert \\mathbf{d}'_{ij} \\rVert} \\\\\n",
    "  \\mathbf{d}'_{ij} &= Q R^{T} K^{-1} Q^{-1} \\mathbf{x}_{ij, cam} \\\\\n",
    "  \\mathbf{x}'_{cam} &=\n",
    "  \\begin{bmatrix}\n",
    "    fx + z p_{x} & fy + z p_{y} & z & 1\n",
    "  \\end{bmatrix}^{T}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_create_plucker_embeddings():\n",
    "    # (...B, H, W, C)\n",
    "    # B = n videos x t frames\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.0000, -1.0000,  1.0000,  3.0000],\n",
       "          [-3.0000, -1.0000,  1.0000,  3.0000],\n",
       "          [-3.0000, -1.0000,  1.0000,  3.0000]],\n",
       "\n",
       "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-2.0000, -2.0000, -2.0000, -2.0000]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-2.0000, -2.0000, -2.0000, -2.0000]]],\n",
       "\n",
       "\n",
       "        [[[-3.0000, -1.0000,  1.0000,  3.0000],\n",
       "          [-3.0000, -1.0000,  1.0000,  3.0000],\n",
       "          [-3.0000, -1.0000,  1.0000,  3.0000]],\n",
       "\n",
       "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-2.0000, -2.0000, -2.0000, -2.0000]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-2.0000, -2.0000, -2.0000, -2.0000]]]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_plucker_embeddings(f, wx, vecs, T):\n",
    "    # TODO We assume images have even width and height\n",
    "    # Input shapes: (B,), (B,), (B, 3, 3), (B, 3, 4)\n",
    "    # vecs contains the right (vecs[b, 0, :]), up (vecs[b, 1, :]), and forward (vecs[b, 2, :]) unit vectors of the camera in the camera frame\n",
    "    R, t = T[:, :, :3], T[:, :, 3] # Shapes: (B, 3, 3), (B, 3)\n",
    "\n",
    "    # TODO wrong, res_x and res_y should come from image\n",
    "    ry, rx = T.shape[-2], T.shape[-1]\n",
    "    wy = wx * (ry / rx) # Shape (B,)\n",
    "\n",
    "    # Creating tensors with indices\n",
    "    i = torch.arange(rx, dtype=torch.float64, device=T.device)\n",
    "    j = torch.arange(ry, dtype=torch.float64, device=T.device)\n",
    "\n",
    "    # Computing displacements\n",
    "    # Shapes: (W,), (H,)\n",
    "    dx = ((i + 0.5) / rx - 0.5)\n",
    "    dy = -((j + 0.5) / ry - 0.5)\n",
    "\n",
    "    dx2 = torch.einsum('b,i->bi', wx, dx) # dx2_bi = wx_b * dx_i\n",
    "    dy2 = torch.einsum('b,j->bj', wy, dy) # dy2_bj = wy_b * dy_j\n",
    "    \n",
    "    # Computing pixel point in camera frame\n",
    "    v1 = torch.einsum('bi,bc->bic', dx2, vecs[:, 0, :]) # v1_bic = dx2_bi * vr_c\n",
    "    v2 = torch.einsum('bj,bc->bjc', dy2, vecs[:, 1, :]) # v2_bjc = dy2_bj * vu_c\n",
    "    v3 = torch.einsum('b,bc->bc', f, vecs[:, 2, :]) # v3_bc = f_b * vf_c\n",
    "\n",
    "    # q_bijc = v1_bic + v2_bjc + v3_bc\n",
    "    q = v1[:, :, None, :] + v2[:, None, :, :] + v3[:, None, None, :] # TODO test speed with unsqueeze\n",
    "    \n",
    "    p = t[:, :, None, None]\n",
    "    l = torch.einsum('bijc,bkc->bkji', q, R) # l_bijk = q_bijc * R_bkc # TODO test speed with unsqueeze\n",
    "    m = torch.cross(p, l, dim=1)\n",
    "    \n",
    "    # Plucker ray embeddings\n",
    "    pl = torch.cat([l, m], dim=1) # Shape: (B, 6, H, W)\n",
    "\n",
    "    return pl\n",
    "\n",
    "B = 2\n",
    "f = torch.tensor(1, device=device).repeat(B)\n",
    "wx = torch.tensor(8, device=device).repeat(B)\n",
    "vecs = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, -1]], dtype=torch.float64, device=device).repeat(B, 1, 1)\n",
    "T = torch.tensor([[1, 0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=torch.float64, device=device).repeat(B, 1, 1)\n",
    "# img = torch.zeros((B, 3, 4, 4), dtype=torch.float64, device=device)\n",
    "# CreatePluckerRayEmbedding()((f, wx, vecs, T, img))[:, 6:, :, :]\n",
    "compute_plucker_embeddings(f, wx, vecs, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_flatten_embeddings(embeddings, p):\n",
    "    # Input shape: (B, C, H, W)\n",
    "    patches = embeddings.unfold(2, p, p).unfold(3, p, p) # Shape: (B, C, H/p, W/p, p, p)\n",
    "    patches = patches.permute(0, 2, 3, 4, 5, 1)\n",
    "    patches = patches.flatten(3, 5).flatten(0, 2) # Shape: (BWH/p^2, Cp^2)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def reverse_patchify_flatten_embeddings(patches, p, W, H):\n",
    "    patches = patches.unflatten(0, (-1, H//p, W//p)).unflatten(3, (p, p, -1))\n",
    "    patches = patches.permute(0, 5, 1, 2, 3, 4)\n",
    "    embeddings = patches.permute(0, 1, 2, 4, 3, 5).reshape((*patches.shape[0:2], H, W))\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "x = torch.arange(2*3*16*16).reshape((2, 3, 16, 16))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        assert torch.equal(x.unfold(2, 4, 4).unfold(3, 4, 4)[:, :, i, j, :, :], x[:, :, i*4:i*4+4, j*4:j*4+4])\n",
    "\n",
    "for T in [1, 2, 4, 8, 16]:\n",
    "    assert torch.equal(x, reverse_patchify_flatten_embeddings(patchify_flatten_embeddings(x, T), T, 16, 16))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptual loss (from [LVSM](https://arxiv.org/pdf/2410.17242) paper that says its from\n",
    "[GS-LRM](https://arxiv.org/pdf/2404.19702) paper that says its from\n",
    "[this paper](https://arxiv.org/pdf/1707.09405) which uses a feature reconstruction loss,\n",
    "introduced in [this paper](https://arxiv.org/pdf/1603.08155))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabma\\Desktop\\Projects\\lvsm\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.), tensor(0., grad_fn=<DivBackward0>), tensor(0., grad_fn=<DivBackward0>), tensor(0., grad_fn=<DivBackward0>), tensor(0.0336, grad_fn=<DivBackward0>), tensor(0.0081, grad_fn=<DivBackward0>), tensor(0.0938, grad_fn=<DivBackward0>), tensor(0.0443, grad_fn=<DivBackward0>), tensor(0.1299, grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "concat() received an invalid combination of arguments - got (list, device=torch.device), but expected one of:\n * (tuple of Tensors tensors, int dim = 0, *, Tensor out = None)\n * (tuple of Tensors tensors, name dim, *, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     23\u001b[0m I \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(shape)\n\u001b[0;32m     24\u001b[0m [\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mperceptual_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperceptual_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperceptual_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     26\u001b[0m     perceptual_loss(perceptual_layers, perceptual_params, torch\u001b[38;5;241m.\u001b[39mzeros(shape), torch\u001b[38;5;241m.\u001b[39mones(shape)),\n\u001b[0;32m     27\u001b[0m     perceptual_loss(perceptual_layers, perceptual_params, torch\u001b[38;5;241m.\u001b[39mones(shape), torch\u001b[38;5;241m.\u001b[39mzeros(shape)),\n\u001b[0;32m     28\u001b[0m     perceptual_loss(perceptual_layers, perceptual_params, torch\u001b[38;5;241m.\u001b[39mzeros(shape), torch\u001b[38;5;241m.\u001b[39mzeros(shape)),\n\u001b[0;32m     29\u001b[0m     perceptual_loss(perceptual_layers, perceptual_params, torch\u001b[38;5;241m.\u001b[39mones(shape), torch\u001b[38;5;241m.\u001b[39mones(shape)),\n\u001b[0;32m     30\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mperceptual_loss\u001b[1;34m(perceptual_layers, perceptual_params, imgs, target_imgs)\u001b[0m\n\u001b[0;32m     10\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mnorm(x1 \u001b[38;5;241m-\u001b[39m x2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (C \u001b[38;5;241m*\u001b[39m H \u001b[38;5;241m*\u001b[39m W))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(losses)\n\u001b[1;32m---> 13\u001b[0m res \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m  perceptual_params)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# res = torch.concat(losses, device=imgs.device).sum()\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[1;31mTypeError\u001b[0m: concat() received an invalid combination of arguments - got (list, device=torch.device), but expected one of:\n * (tuple of Tensors tensors, int dim = 0, *, Tensor out = None)\n * (tuple of Tensors tensors, name dim, *, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "def perceptual_loss(perceptual_layers, perceptual_params, imgs, target_imgs):\n",
    "    # \n",
    "    # TODO Fix so that the implementation is right (use every layer and use 1 norm instead of 2 and use hyperparameters for each layer)\n",
    "    x1, x2 = imgs, target_imgs\n",
    "    losses = []\n",
    "\n",
    "    for l in perceptual_layers:\n",
    "        x1, x2 = l(x1), l(x2)\n",
    "        C, H, W = x1.shape[-3], x1.shape[-2], x1.shape[-1]\n",
    "        losses.append(torch.norm(x1 - x2, p=2, dim=-1).sum() / (C * H * W))\n",
    "\n",
    "    print(losses)\n",
    "    res = (torch.concat(losses, device=imgs.device) *  perceptual_params).sum()\n",
    "    # res = torch.concat(losses, device=imgs.device).sum()\n",
    "    return res\n",
    "\n",
    "perceptual_model = torchvision.models.convnext_tiny(torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "perceptual_model\n",
    "perceptual_model = perceptual_model.features\n",
    "perceptual_layers = [lambda x: x] + list(perceptual_model)\n",
    "perceptual_params = torch.ones(8, dtype=torch.float64)\n",
    "shape = (4, 3, 64, 64)\n",
    "I = torch.rand(shape)\n",
    "[\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, I, I),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.zeros(shape), torch.ones(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.ones(shape), torch.zeros(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.zeros(shape), torch.zeros(shape)),\n",
    "    perceptual_loss(perceptual_layers, perceptual_params, torch.ones(shape), torch.ones(shape)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO document these in obsidian\n",
    "\n",
    "TODO where to send data to device: send it when loading from dataloader\n",
    "\n",
    "```py\n",
    "for x_data, y_data in train_dataloader:\n",
    "    x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "```\n",
    "\n",
    "check\n",
    "pytorch lightning dataloader for device\n",
    "https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
    "    LightningDataModule.transfer_batch_to_device()\n",
    "https://github.com/Lightning-AI/pytorch-lightning/issues/3341\n",
    "\n",
    "TODO how does it compute gradients for a batch? it keeps aggregating the gradient for individual elements of the batch until we use step and zero_grad, then a new batch starts\n",
    "\n",
    "TODO\n",
    "- first layer creates batch tensor with multiple source and target images and a second tensor with indices of target images in first tensor\n",
    "- second layer (plucker embeddings) receives only batch tensor and outputs plucker ray embeddings\n",
    "- third layer projects embeddings onto linear tokens\n",
    "- fourth layer is transformer\n",
    "- then only in fifth layer we use the target images' indices tensor to get resulting images and compare to actual target images\n",
    "\n",
    "TODO check tensor views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLVSM\u001b[39;00m(\u001b[43mL\u001b[49m\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, l, p, ):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "class LVSM(nn.Module):\n",
    "    def __init__(self, p, d, l, C, N, h):\n",
    "        # p is patch size, d is latent size, l is number of latent tokens, C is number of channels in each image\n",
    "        # N is number of encoder/decoder layers, h is number of attention heads\n",
    "        super().__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.l = l\n",
    "        self.C = C\n",
    "        \n",
    "        self.linear_in = nn.Linear(in_features=(C + 6) * p * p, out_features=d)\n",
    "        self.target_linear = nn.Linear(in_features=6 * p * p, out_features=d)\n",
    "        self.layer_out = nn.Sequential([\n",
    "            nn.Linear(in_features=d, out_features=(C + 6) * p * p),\n",
    "            nn.Sigmoid(),\n",
    "        ])\n",
    "        \n",
    "        xformer_config = [\n",
    "            {\n",
    "                \"reversible\": False,\n",
    "                \"block_type\": \"encoder\",\n",
    "                \"num_layers\": N,\n",
    "                \"dim_model\": d,\n",
    "                \"residual_norm_style\": \"pre\",\n",
    "                # \"position_encoding_config\": {\n",
    "                #     \"name\": \"sine\",\n",
    "                #     \"seq_len\": self.hparams.block_size,\n",
    "                # },\n",
    "                \"multi_head_config\": {\n",
    "                    \"num_heads\": h,\n",
    "                    \"residual_dropout\": 0.1,\n",
    "                    \"use_rotary_embeddings\": False,\n",
    "                    \"attention\": {\n",
    "                        \"name\": self.hparams.attention,\n",
    "                        \"dropout\": 0.1,\n",
    "                        \"causal\": False,\n",
    "                        \"seq_len\": self.hparams.block_size,\n",
    "                        \"num_rules\": self.hparams.n_head,\n",
    "                    },\n",
    "                },\n",
    "                \"feedforward_config\": {\n",
    "                    \"name\": \"MLP\",\n",
    "                    \"dropout\": 0.1,\n",
    "                    \"activation\": \"gelu\",\n",
    "                    \"hidden_layer_multiplier\": self.hparams.hidden_layer_multiplier,\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        config = xFormerConfig(xformer_config)\n",
    "        config.weight_init = 'small'\n",
    "        self.model = xFormer.from_config(config)\n",
    "        \n",
    "    def forward(self, source_batch, target_batch):\n",
    "        # TODO We assume p divides W and H\n",
    "        # TODO Here we compute the plucker embeddings in the whole image before breaking it into patches, but in the paper they break the image first, then compute the patches later (test patching before and after computing plucker ray embeddings)\n",
    "        # TODO we assume batch size 1, so B will be the number of input images of that batch\n",
    "        # Shapes: (B,), (B,), (B, 3, 3), (B, 3, 4), (B, C, H, W)\n",
    "        f, wx, vecs, T, imgs = source_batch\n",
    "        f2, wx2, vecs2, T2 = target_batch\n",
    "        W, H = imgs.shape[3], imgs.shape[2]\n",
    "\n",
    "        # Sources and target plucker rays\n",
    "        # Shapes: (B, C + 6, H, W), (1, 6, H, W)\n",
    "        source_pl = compute_plucker_embeddings(f, wx, vecs, T)\n",
    "        source_pl = torch.cat([imgs, source_pl], dim=1)\n",
    "        target_pl = compute_plucker_embeddings(f2, wx2, vecs2, T2)\n",
    "        \n",
    "        # Creates and flattens patches\n",
    "        source_flattened_patches = patchify_flatten_embeddings(source_pl, self.p)\n",
    "        target_flattened_patches = patchify_flatten_embeddings(target_pl, self.p)\n",
    "\n",
    "        # Linear transformation so that they have same shape\n",
    "        # TODO test directly sending them flattened instead of using linear. in that case, the target patches would need images, but these would be either zeros or learned latent tokens\n",
    "        source_tokens = self.linear_in(source_flattened_patches)\n",
    "        target_tokens = self.target_linear(target_flattened_patches)\n",
    "        \n",
    "        # Generates output\n",
    "        # TODO use inheritance use abstract function here and specialize into enc-dec and dec-only\n",
    "        output = self.model(torch.cat([source_tokens, target_tokens]))\n",
    "        \n",
    "        # Generates target embeddings back from output\n",
    "        # Source outputs are discarded\n",
    "        target_tokens_out = output[source_tokens.shape[0]:]\n",
    "        target_flattened_patches_out = self.layer_out(target_tokens_out)\n",
    "        target_embs_out = reverse_patchify_flatten_embeddings(target_flattened_patches_out, self.p, W, H)\n",
    "        \n",
    "        # TODO test using linear instead of just stripping off the final plucker embeddings from the tokens\n",
    "        out_imgs = target_embs_out[:, :-6, :, :]\n",
    "        return out_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 7, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "a(torch.randn(14, 7, 10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(L.LightningModule):\n",
    "    def __init__(self, perceptual_params):\n",
    "        super().__init__()\n",
    "        # perceptual_params is the parameters for the weighted perceptual loss of the rendered images\n",
    "        \n",
    "        self.lvsm = LVSM() #TODO\n",
    "\n",
    "        # Perceptual loss layers\n",
    "        # self.perceptual_layers = torchvision.models.vgg19(torchvision.models.VGG19_Weights.DEFAULT).features\n",
    "        perceptual_model = torchvision.models.convnext_tiny(torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT)\n",
    "        perceptual_model.eval()\n",
    "        self.perceptual_layers = perceptual_model.features\n",
    "        self.perceptual_layers = [lambda x: x] + list(self.perceptual_layers) # This makes the first layer the identity (this layer is used to compute the MSE loss)\n",
    "        self.perceptual_params = torch.tensor(perceptual_params)\n",
    "\n",
    "    def step(self, batch):\n",
    "        f, wx, vecs, T, imgs = batch\n",
    "\n",
    "        # For each batch, we choose the last image as the target image\n",
    "        # TODO create multiple targets instead of a single one\n",
    "        # TODO to do this, probably we would pass the source tokens and copy the transformer state at that point and create target tokens for each image from there\n",
    "        source_batch = f[:-1], wx[:-1], vecs[:-1], T[:-1], imgs[:-1]\n",
    "        target_batch = f[-1:], wx[-1:], vecs[-1:], T[-1:]\n",
    "        target_imgs = imgs[-1:]\n",
    "        \n",
    "        gen_imgs = self.lvsm(source_batch, target_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvsm = LVSM(16, 20, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 17)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('poses_bounds.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetStandardizer(nn.Module):\n",
    "    # axis_permutation shape (3, 3) converts permuted/negative axes to <TODO standard>\n",
    "    # intrinsic_01_range whether the intrinsics map to the 0-1 range\n",
    "    # time_scaling = 1/FPS\n",
    "    def __init__(self, axis_permutation, intrinsic_01_range, time_scaling):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, I, E, K, t):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4748, 0.4749, 0.4750, 0.4751],\n",
       "          [0.4749, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4749, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4749, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4750, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4750, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4750, 0.4750, 0.4751, 0.4751],\n",
       "          [0.4750, 0.4750, 0.4751, 0.4751]],\n",
       "\n",
       "         [[0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719],\n",
       "          [0.5719, 0.5719, 0.5719, 0.5719]],\n",
       "\n",
       "         [[0.6690, 0.6688, 0.6688, 0.6687],\n",
       "          [0.6689, 0.6688, 0.6688, 0.6687],\n",
       "          [0.6689, 0.6688, 0.6688, 0.6687],\n",
       "          [0.6689, 0.6688, 0.6688, 0.6687],\n",
       "          [0.6688, 0.6688, 0.6687, 0.6687],\n",
       "          [0.6688, 0.6688, 0.6687, 0.6687],\n",
       "          [0.6688, 0.6688, 0.6687, 0.6687],\n",
       "          [0.6688, 0.6688, 0.6687, 0.6687]]],\n",
       "\n",
       "\n",
       "        [[[0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506],\n",
       "          [0.5506, 0.5506, 0.5506, 0.5506]],\n",
       "\n",
       "         [[0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769],\n",
       "          [0.5769, 0.5769, 0.5769, 0.5769]],\n",
       "\n",
       "         [[0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033],\n",
       "          [0.6033, 0.6033, 0.6033, 0.6033]]],\n",
       "\n",
       "\n",
       "        [[[0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620],\n",
       "          [0.5620, 0.5620, 0.5620, 0.5620]],\n",
       "\n",
       "         [[0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772],\n",
       "          [0.5772, 0.5772, 0.5772, 0.5772]],\n",
       "\n",
       "         [[0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924],\n",
       "          [0.5924, 0.5924, 0.5924, 0.5924]]],\n",
       "\n",
       "\n",
       "        [[[0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666],\n",
       "          [0.5666, 0.5666, 0.5666, 0.5666]],\n",
       "\n",
       "         [[0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773],\n",
       "          [0.5773, 0.5773, 0.5773, 0.5773]],\n",
       "\n",
       "         [[0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880],\n",
       "          [0.5880, 0.5880, 0.5880, 0.5880]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PoseEncoder(nn.Module):\n",
    "    # intrinsic_01_range = if true, intrinsics map coords into points in sensor in range 0-1, otherwise, maps to range 0-W or 0-H\n",
    "    def __init__(self, p, n_oct, intrinsic_01_range=True):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.n_oct = n_oct\n",
    "        self.intrinsic_01_range = intrinsic_01_range\n",
    "\n",
    "    # I = images, HW = tuple with height and width\n",
    "    # Set both if image has been resized, specifying original image height and width in HW\n",
    "    # We assume images are already resized (always resize them maintaining aspect ratio)\n",
    "    def forward(self, E, K, t, I = None, HW = None):\n",
    "        assert (I == None) ^ (HW == None), 'Either I or HW or both should be set'\n",
    "        \n",
    "        #TODO corrige hw\n",
    "        #TODO tem que retornar quanto de padding teve pra tirar o padding na comparacao da loss function\n",
    "        #TODO na verdade no lugar de retornar o padding ja retorna a visao prevista com padding retirado no modelo final\n",
    "\n",
    "        HW = I.shape[-2:] if HW == None else HW\n",
    "        R, T = E[:, :3, :3], E[:, :3, 3]\n",
    "        Kinv = K.inverse()\n",
    "        \n",
    "        # Pads the input so that it is divisible by 'p'\n",
    "        pad = [((self.p - i) % self.p) for i in HW]\n",
    "        pad_s = [i // 2 for i in pad]\n",
    "        pad = (pad_s[1], pad[1] - pad_s[1], pad_s[0], pad[0] - pad_s[0])\n",
    "        I = F.pad(I, pad, 'constant', 0)\n",
    "\n",
    "        # Creates vectors for each pixel in screen\n",
    "        # No need to unflip y axis since it being flipped does not affect the topological structure of the representation\n",
    "        ranges = [torch.arange(l, dtype=torch.float64) - o + 0.5 for o, l in zip(pad_s, I.shape[-2:])]\n",
    "        vecs = torch.meshgrid(*ranges, indexing='ij')\n",
    "        if self.intrinsic_01_range:\n",
    "            vecs = [v / l for v, l in zip(vecs, HW)]\n",
    "        vecs = torch.concat([torch.stack([*vecs[::-1]]), torch.ones((1, *vecs[0].shape))], dim=-3)\n",
    "        vecs = einops.repeat(vecs, 'c h w -> b c h w', b=I.shape[-4])\n",
    "\n",
    "        # Computes view rays\n",
    "        o = -einops.einsum(R, T, 'b h w, b h -> b w')  # -R^T t\n",
    "        o = einops.repeat(o, 'b c -> b c h w', h=I.shape[-2], w=I.shape[-1]) # repeat o for each vec\n",
    "        d = einops.einsum(R.to(torch.float64), Kinv.to(torch.float64), vecs, 'b x1 c2, b x1 c, b c h w -> b c2 h w') # R^T K^-1 x_ij,cam\n",
    "        d = d / einx.sum('b [c] h w -> b 3 h w', d * d).sqrt() # normalize d\n",
    "        \n",
    "        \n",
    "        return d\n",
    "\n",
    "a = PoseEncoder(p=4, n_oct=6)\n",
    "a.forward(torch.arange(64).reshape((4, 4, 4)) + 0.0, torch.linalg.inv(torch.arange(9).reshape((3, 3)) + 4.0).unsqueeze(0).repeat((4, 1, 1)), 0, torch.ones((4, 2, 5, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change to RawDVST, create DVST that also has CNN to reduce dims and PoseWrapper to add a pose estimator to both\n",
    "class DVST(nn.Module):\n",
    "    # not specified: H, W, C, N_{context}\n",
    "    # n_heads has to divide d_lat\n",
    "    # p has to divide H and W (padding, cropping and resizing)\n",
    "    def __init__(self, N_enc, N_dec, n_heads, d_lat, e_ff, n_lat, p, n_oct):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_lat % n_heads == 0, \"n_heads should divide d_lat\"\n",
    "\n",
    "        self.N_enc = N_enc\n",
    "        self.N_dec = N_dec\n",
    "        self.n_heads = n_heads\n",
    "        self.d_lat = d_lat\n",
    "        self.e_ff = e_ff\n",
    "        self.n_lat = n_lat\n",
    "        self.p = p\n",
    "        self.n_oct = n_oct\n",
    "        \n",
    "        self.pose_encoder = PoseEncoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
